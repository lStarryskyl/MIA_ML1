{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5dd4668",
   "metadata": {},
   "source": [
    "# Ensemble Models\n",
    "\n",
    "One of the latest trends in artificial intelligence modelling can be summarised as \"knowledge of the whole or the crowd\". What this somewhat familiar phrase defines is the use of a multitude of so-called \"weak\" models in a meta-classifier. The aim is to generate a \"strong\" model based on the knowledge extracted by the \"weak\" models. For example, although it will be detailed later, multiple, much simpler Decision Trees are developed in a Random Forest. The combination of these ones in the Random Forest exceeds the performance of any of the individual models. The models that emerge in this way, as meta-classifiers or meta-regressors, are generically called **Ensemble models**.\n",
    "\n",
    "Is is worth mentioned that these models may not be limited only to decision trees, but may instead be composed of any type of machine learning model that has been seen previously. They can even be mixed models where not all models have been obtained in the same way, but can be created through the combined use of several techniques such as K-NN, SVM, etc. Thus, the first criteria to classifify the ensemble models would be if they are homogeneous or heterogeneous models. However this is not the only criteria to classifity the ensemble models, in this unit, we will explore various ways of generating the models and how to combine them later on. We will also take a closer look at two of the most common techniques within ensemble models such as Random Forest and _XGBoost_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e861d",
   "metadata": {},
   "source": [
    "First of all, ensure that the required packages are installed. Therefore, you should only execute the folowing cell the fist time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b736500",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "# Ensure required packages (uncomment to install if needed)\n",
    "Pkg.add([\n",
    "    \"MLJ\", \n",
    "    \"MLJBase\", \n",
    "    \"MLJModels\", \n",
    "    \"MLJEnsembles\", \n",
    "    \"MLJLinearModels\", \n",
    "    \"DecisionTree\", \n",
    "    \"MLJDecisionTreeInterface\", \n",
    "    \"NaiveBayes\", \n",
    "    \"EvoTrees\", \n",
    "    \"CategoricalArrays\", \n",
    "    \"Random\",\n",
    "    \"LIBSVM\",           \n",
    "    \"Plots\",            \n",
    "    \"MLJModelInterface\", \n",
    "    \"CSV\",              \n",
    "    \"DataFrames\",       \n",
    "    \"UrlDownload\",      \n",
    "    \"XGBoost\"    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8f73db",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Unlike the first tutorials, where the iris flower problem has been used as a benchmark, in this tutorial we will use a different one. The problem is also included in the UCI repository, although it is also small, the number of variables increases significantly and therefore it will give us some more room to explore. Specifically, it is a classic machine learning problem, which is informally called Rock or Mine? It is a small database consisting of 111 patterns corresponding to rocks and 97 to water mines (simulated as metal cylinders). Each of the patterns consists of 60 numerical measurements corresponding to a section of the sonar sequences. These values are already between 0.0 and 1.0, although it is worth normalising them to be on the safe side. These measurements represent the energy value of different wavelength ranges for a certain period of time.\n",
    "\n",
    "We are going to use a couple of new packages in the process, more specificly, [DataFrames.jl](https://juliaai.github.io/DataScienceTutorials.jl/data/dataframe/) and [UrlDownload.jl](https://github.com/Arkoniak/UrlDownload.jl). Therefore, first thing first, ensure that the packages are correcly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"UrlDownload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad05475",
   "metadata": {},
   "source": [
    "After that, the data will be downloaded if they are not already available, for which the following code can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "using UrlDownload\n",
    "using DataFrames\n",
    "using CSV\n",
    "using CategoricalArrays\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
    "data = urldownload(url, true, format=:CSV, header=false) |> DataFrame\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ebf504",
   "metadata": {},
   "source": [
    "As it can be seen in the previos line, we have downloaded de data and pipe it, with the operator `|>`, into the function DataFrame. This is going to create an structure simular to a database table which is particular convinient to check for missing values or the ranges of the different variables. In fact, the library makes it particularly easy to deal with missing values with functions to fullfill or remove the samples with non-valid measures. However it is too long to see every single variable on the output report, if some queries are made we can identify  that here is no missing values. Additionally no variable is over 1.0 but some of them are not normalized. A similar structure can be found in other languages, like R or Python.\n",
    "\n",
    "As an example, of this process lets make the an additional column in order to convert to categorical the las column 60 which has a **M** for each Mine and an **R** for each sample of rock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "insertcols!(data, :Mine => data[:, 61].==\"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa393f",
   "metadata": {},
   "source": [
    "Once the data is loaded in the DataFrame for the checking proposes and that any posible process has been applied on the data. As in previous tutorials, the data has to be put on a Matrix form, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Matrix(data[!, 1:60]);\n",
    "output_data = data[!, :Mine];\n",
    "\n",
    "@assert input_data isa Matrix\n",
    "@assert output_data isa BitVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148c7",
   "metadata": {},
   "source": [
    "It is worth to mention that in a DataFrame when a set of lines is queried such as in the case of the `X`, the results is also a DataFrame. Therefore, in order to applied the remaining operations it is needed to applied the `Matrix` function to retrive a matrix where the previous operations can be used as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272d3b3",
   "metadata": {},
   "source": [
    "### Question 7.1\n",
    "\n",
    "> ❓ Now, the data is loaded and converted to the usual types. Now you should be able to apply in the next section and make asplit of the dataset in two subset, test and training, and apply the corresponding normalization. Put the code on the following section to perform both operations. *Tip: Due to the preparation for MLJ models, read the notes at the end of the document*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e02ee",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input, train_output, test_input, test_output = #TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe5ef",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "As mentioned above, ensembles are a set of \"weaker\" classifiers that allow us to later overcome their limits by joining them together. That is why, before starting with ensembles, it will be necessary to have some reference models that will later be joined together in a meta-classifier. In the following example, some simple models, imlemented with `MLJ` library, are trained: an SVM with RBF kernel, a Linear Regression, a Naïve Bayes and a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "\n",
    "# Load models (MLJ will prompt to add missing packages the first time you run these)\n",
    "SVC = @load ProbabilisticSVC pkg=LIBSVM\n",
    "LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels\n",
    "DecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree\n",
    "GaussianNBClassifier = @load GaussianNBClassifier pkg=NaiveBayes\n",
    "\n",
    "#Define the models to train\n",
    "models = Dict(\n",
    "    \"SVM\" => SVC(),\n",
    "    \"LR\"  => LogisticClassifier(),\n",
    "    \"DT\"  => DecisionTreeClassifier(max_depth=4),\n",
    "    \"NB\"  => GaussianNBClassifier(),\n",
    ")\n",
    "\n",
    "base_models=  [ model for (name, model) in models]\n",
    "\n",
    "machines = Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a595d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the training for each model and calculate the test values (accuracy)\n",
    "for (name, model) in models\n",
    "    machines[name] = machine(model, train_input, train_output) |> fit!\n",
    "    acc = MLJ.accuracy(predict_mode(machines[name], test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ecb8c",
   "metadata": {},
   "source": [
    "## Combining weak models in an ensemble\n",
    "\n",
    "When it comes to combining the models, there are different strategies depending on the task of the model, i.e. whether we are classifying or regressing. In this particular case we are going to focus on classification, although for regression it would be similar, but the continuous nature of the values should be taken into account when combining the outputs.\n",
    "\n",
    "Regarding the combination of the classification, there are mainly two ways to combine the outputs of several classifiers. These combinations are called Majority voting and Weighted majority voting, also known as Soft Voting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using MLJBase\n",
    "using MLJModelInterface\n",
    "\n",
    "# ===================================================\n",
    "# DEFINITION OF VOTINGCLASSIFIER COMPATIBLE WITH MLJ\n",
    "# ===================================================\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier <: Probabilistic\n",
    "\n",
    "An ensemble classifier that combines predictions from multiple base models using voting strategies.\n",
    "\n",
    "# Fields\n",
    "- `models::Vector{Probabilistic}`: Vector of base probabilistic models to be combined\n",
    "- `voting::Symbol`: Voting strategy, either `:hard` (majority vote) or `:soft` (averaged probabilities)\n",
    "- `weights::Union{Nothing, Vector{Float64}}`: Optional weights for each model. If `nothing`, all models have equal weight. Weights are automatically normalized to sum to 1.0.\n",
    "\n",
    "# Examples\n",
    "```julia\n",
    "# Equal weights (default)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier()],\n",
    "    voting=:soft\n",
    ")\n",
    "\n",
    "# Custom weights (will be normalized automatically)\n",
    "voting_clf = VotingClassifier(\n",
    "    models=[LogisticClassifier(), DecisionTreeClassifier(), RandomForestClassifier()],\n",
    "    voting=:hard,\n",
    "    weights=[5, 3, 2]  # Will be normalized to [0.5, 0.3, 0.2]\n",
    ")\n",
    "```\n",
    "\"\"\"\n",
    "mutable struct VotingClassifier <: Probabilistic   # Models must be probabilistic, inherited from MLJBase\n",
    "    models::Vector{Probabilistic}\n",
    "    voting::Symbol  # :hard or :soft\n",
    "    weights::Union{Nothing, Vector{Float64}}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "\n",
    "Constructor for VotingClassifier.\n",
    "\n",
    "# Arguments\n",
    "- `models::Vector{Probabilistic}=Probabilistic[]`: Base models to combine\n",
    "- `voting::Symbol=:hard`: Voting strategy (`:hard` or `:soft`)\n",
    "- `weights::Union{Nothing, Vector{<:Real}}=nothing`: Weights for each model. Automatically normalized to sum to 1.0.\n",
    "\n",
    "# Throws\n",
    "- `AssertionError`: If voting is not `:hard` or `:soft`\n",
    "- `AssertionError`: If weights length doesn't match models length\n",
    "- `AssertionError`: If all weights are zero or negative\n",
    "\"\"\"\n",
    "function VotingClassifier(; models=Probabilistic[], voting=:hard, weights=nothing)\n",
    "    @assert voting in [:hard, :soft] \"The only possible labels are :hard or :soft\"\n",
    "    \n",
    "    normalized_weights = nothing\n",
    "    if weights !== nothing\n",
    "        @assert length(weights) == length(models) \"Number of weights must match number of models\"\n",
    "        @assert all(w >= 0 for w in weights) \"All weights must be non-negative\"\n",
    "        \n",
    "        # Normalize weights to sum to 1.0\n",
    "        normalized_weights = Float64.(weights) ./ sum(weights)\n",
    "    end\n",
    "    \n",
    "    return VotingClassifier(models, voting, normalized_weights)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "\n",
    "Fit the VotingClassifier by training each base model on the provided data.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `verbosity::Int`: Verbosity level for training output\n",
    "- `X`: Training features (table format)\n",
    "- `y`: Training target (categorical vector)\n",
    "\n",
    "# Returns\n",
    "- `fitresults`: Vector of trained machines (one per base model)\n",
    "- `cache`: Nothing (no caching implemented)\n",
    "- `report`: Named tuple with training information (number of models, voting strategy, and normalized weights)\n",
    "\"\"\"\n",
    "function MLJModelInterface.fit(model::VotingClassifier, verbosity::Int, X, y)\n",
    "    # Train each base model\n",
    "    fitresults = []\n",
    "    for base_model in model.models\n",
    "        model_copy = deepcopy(base_model)\n",
    "        mach = machine(model_copy, X, y)\n",
    "        fit!(mach, verbosity=0)\n",
    "        push!(fitresults, mach)\n",
    "    end\n",
    "    \n",
    "    # Save necessary information\n",
    "    cache = nothing\n",
    "    report = (n_models=length(model.models), voting=model.voting, weights=model.weights)\n",
    "    \n",
    "    return fitresults, cache, report\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class labels using hard voting (majority vote with optional weights).\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Categorical vector of predicted class labels based on (weighted) majority voting\n",
    "\n",
    "# Details\n",
    "Each base model votes for a class. If weights are provided, each vote is multiplied by its \n",
    "corresponding weight. The class with the highest (weighted) vote count is selected.\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict_mode(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines = fitresult\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    predictions = [predict_mode(mach, Xnew) for mach in machines]\n",
    "    \n",
    "    # Get all unique classes\n",
    "    all_classes = unique(vcat([unique(p) for p in predictions]...))\n",
    "    n_samples = length(predictions[1])\n",
    "    n_models = length(machines)\n",
    "    \n",
    "    # Determine weights (equal if not specified)\n",
    "    weights = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "    \n",
    "    # Weighted voting\n",
    "    ensemble_pred = Vector{eltype(predictions[1])}(undef, n_samples)\n",
    "    \n",
    "    for i in 1:n_samples\n",
    "        # Count weighted votes for each class\n",
    "        vote_counts = Dict{eltype(predictions[1]), Float64}()\n",
    "        for class in all_classes\n",
    "            vote_counts[class] = 0.0\n",
    "        end\n",
    "        \n",
    "        for (j, pred) in enumerate(predictions)\n",
    "            vote_counts[pred[i]] += weights[j]\n",
    "        end\n",
    "        \n",
    "        # Select class with maximum weighted votes\n",
    "        ensemble_pred[i] = argmax(vote_counts)\n",
    "    end\n",
    "    \n",
    "    return categorical(ensemble_pred)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "\n",
    "Predict class probabilities using the specified voting strategy.\n",
    "\n",
    "# Arguments\n",
    "- `model::VotingClassifier`: The voting classifier instance\n",
    "- `fitresult`: Vector of trained machines from fit\n",
    "- `Xnew`: New data to predict on\n",
    "\n",
    "# Returns\n",
    "- Vector of `UnivariateFinite` distributions representing class probabilities\n",
    "\n",
    "# Details\n",
    "- For `:hard` voting: Returns deterministic predictions wrapped in UnivariateFinite (with optional weights)\n",
    "- For `:soft` voting: Averages probability distributions from all base models using weights\n",
    "\"\"\"\n",
    "function MLJModelInterface.predict(model::VotingClassifier, fitresult, Xnew)\n",
    "    machines = fitresult\n",
    "    \n",
    "    result = if model.voting == :hard\n",
    "        # For hard voting, return deterministic predictions\n",
    "        UnivariateFinite(predict_mode(model, fitresult, Xnew))\n",
    "    else\n",
    "        # Soft voting: weighted average of probabilities\n",
    "        all_predictions = [predict(mach, Xnew) for mach in machines]\n",
    "        \n",
    "        # Get class levels\n",
    "        first_pred = all_predictions[1][1]\n",
    "        class_levels = MLJBase.classes(first_pred)\n",
    "        n_classes = length(class_levels)\n",
    "        n_samples = length(all_predictions[1])\n",
    "        n_models = length(machines)\n",
    "        \n",
    "        # Determine weights (equal if not specified)\n",
    "        weights = model.weights === nothing ? fill(1.0/n_models, n_models) : model.weights\n",
    "        \n",
    "        # Weighted average of probabilities\n",
    "        avg_probs = zeros(n_samples, n_classes)\n",
    "        for (model_idx, preds) in enumerate(all_predictions)\n",
    "            for i in 1:n_samples\n",
    "                for (j, level) in enumerate(class_levels)\n",
    "                    avg_probs[i, j] += weights[model_idx] * pdf(preds[i], level)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # Create UnivariateFinite distributions with weighted averaged probabilities\n",
    "        [UnivariateFinite(class_levels, avg_probs[i, :]) for i in 1:n_samples]\n",
    "    end\n",
    "    \n",
    "    return result\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "Model metadata registration for VotingClassifier.\n",
    "\n",
    "Specifies input/output types and capabilities for MLJ integration.\n",
    "\"\"\"\n",
    "MLJModelInterface.metadata_model(VotingClassifier,\n",
    "    input_scitype=Table(Continuous),\n",
    "    target_scitype=AbstractVector{<:Finite},\n",
    "    supports_weights=false,\n",
    "    load_path=\"VotingClassifier\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2b2cf",
   "metadata": {},
   "source": [
    "## Majority Voting\n",
    "Although also known as Hard Voting, as the name suggests, they are based on selecting the most voted option among the predicted ones among the different models. Each model casts a deterministic vote or prediction. The final class or prediction is the one that receives the most votes or the average among the results. It’s equivalent to a “democratic election” where each model/expert has one vote, and the most-voted option wins. In this way, the problem could be solved taking into account different results or points of view on the problem. \n",
    "\n",
    "### Example\n",
    "\n",
    "With 3 classifiers predicting a pattern:\n",
    "\n",
    "* SVM predicts: **Mine**\n",
    "\n",
    "* Logistic Regression predicts: **Rock**\n",
    "\n",
    "* Naive Bayes predicts: **Rock**\n",
    "\n",
    "**Result:** Rock (2 votes vs 1 vote) ✓\n",
    "\n",
    "See an example in the code below of constructing such a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Hard Voting)\"] = VotingClassifier(estimators = base_models, voting=:hard)\n",
    "machines[\"Ensemble (Hard Voting)\"] = machine(models[\"Ensemble (Hard Voting)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c3971c",
   "metadata": {},
   "source": [
    "The main problem is that we rely equally on all models when deciding on the response class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d50e61",
   "metadata": {},
   "source": [
    "## Soft Voting (Weighted Probability Voting)\n",
    "\n",
    "As mentioned in the previous section, one of the problems of the classical *emsemble* model is that all outcomes are weighted equally and in each of the \"weak\" models only the most voted option is taken into account. To solve this, **Soft Voting** propsose the use of the **probabilities** that each classifier assigns to each class, instead of just the predicted class. The final result is obtained by averaging these probabilities and selecting the class with the highest average probability.\n",
    "\n",
    "### Example without weights (all models equally important)\n",
    "\n",
    "| Classifier    | P(Mine) | P(Rock) |\n",
    "|--------------|---------|---------|\n",
    "| SVM          | 0.9     | 0.1     |\n",
    "| LR           | 0.3     | 0.7     |\n",
    "| NB           | 0.2     | 0.8     |\n",
    "| **Average ** | **0.47**| **0.53**|\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (0.9 + 0.3 + 0.2) / 3 = 0.47\n",
    "- P(Rock) = (0.1 + 0.7 + 0.8) / 3 = 0.53\n",
    "\n",
    "**Result** Rock (highest average probability) ✓\n",
    "\n",
    "**Advantage over Hard Voting:** Even though SVM is very confident about Mine (0.9), the other two models are quite confident about Rock (0.7 and 0.8). Soft Voting captures this confidence information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b01ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the metaclassifier based on the base_models\n",
    "models[\"Ensemble (Soft Voting - Equal)\"] = VotingClassifier( models = base_models, voting = :soft, weights = nothing) # All models equally weighted\n",
    "machines[\"Ensemble (Soft Voting - Equal)\"] = machine(models[\"Ensemble (Soft Voting - Equal)\"], train_input, train_output) |> fit!\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78117f",
   "metadata": {},
   "source": [
    "#### Weigthed Soft Voting \n",
    "Although it could improve because the confidence is taken into account, the main issue is the equality among the models. To solve this, one of the proposals is the use of a weighting in the decision.In many cases, we know that some models perform better than others. For example, if the SVM has an accuracy of 85% and the others are around 70%, we should give more importance to the SVM. This is achieved through weights. In the soft voting, the weights multiply each model’s probabilities before averaging them. Mathematically: \n",
    "\n",
    "$$P(clase) = \\frac{\\sum_{i=1}^{n} w_i \\cdot P_i(clase)}{\\sum_{i=1}^{n} w_i}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $w_i$ = weight of model $i$\n",
    "\n",
    "* $P_i(class)$ = probability assigned by model $i$ to that class\n",
    "\n",
    "* $n$ = number of models\n",
    "\n",
    "Going along with the same example, imagine that we want to increase the importance of the SVM\n",
    "### Example with weights [2, 1, 1] (double weight for SVM)\n",
    "\n",
    "| Classifier   | Weight | P(Mine) | P(Rock)| Mine Contribution | Rock Contribution |\n",
    "|--------------|-------|---------|---------|-------------------|-------------------|\n",
    "| SVM          | 2     | 0.9     | 0.1     | 2 × 0.9 = 1.8     | 2 × 0.1 = 0.2     |\n",
    "| LR           | 1     | 0.3     | 0.7     | 1 × 0.3 = 0.3     | 1 × 0.7 = 0.7     |\n",
    "| NB           | 1     | 0.2     | 0.8     | 1 × 0.2 = 0.2     | 1 × 0.8 = 0.8     |\n",
    "| **Sum**      | 4     |         |         | **2.3**           | **1.7**           |\n",
    "| **Weighted Avg.** |  |         |         | **0.575**         | **0.425** |\n",
    "\n",
    "**Calculation:**\n",
    "- P(Mine) = (1.8 + 0.3 + 0.2) / 4 = 2.3 / 4 = 0.575\n",
    "- P(Rock) = (0.2 + 0.7 + 0.8) / 4 = 1.7 / 4 = 0.425\n",
    "\n",
    "**Result:** Mine (highest weighted probabilit) ✓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260da3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"Ensemble (Soft Voting - Weighted)\"] = VotingClassifier(estimators = base_models, voting=:soft,weights=[1,2,2,1])\n",
    "machines[\"Ensembles (Soft Voting - Weighted)\"] = machine(models[\"Ensemble (Soft Voting - Weighted)\"],train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ee5d",
   "metadata": {},
   "source": [
    "## When to Use Each Strategy\n",
    "### Hard Voting\n",
    "\n",
    "- Models that only output categorical predictions (no probabilities)\n",
    "\n",
    "- When all models are equally reliable\n",
    "\n",
    "- Simpler and faster\n",
    "\n",
    "### Soft Voting (no weights)\n",
    "\n",
    "- Models that output probabilities\n",
    "\n",
    "- When all models perform similarly\n",
    "\n",
    "- Captures confidence in each prediction\n",
    "\n",
    "### Weighted Soft Voting\n",
    "\n",
    "- When some models are clearly better\n",
    "\n",
    "- When you want to give more importance to specific models\n",
    "\n",
    "- Weights can be based on:\n",
    "\n",
    "    - Validation accuracy\n",
    "    - Known model expertise\n",
    "    - F1-score or another relevant metric\n",
    "\n",
    "In order to chose the weights there are several strategies being the most importan: \n",
    "\n",
    "    1. Manual (based on prior knowledge)\n",
    "    2. Based on validation accuracy\n",
    "    3. Optimization via grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be268b",
   "metadata": {},
   "source": [
    "### Question 7.2\n",
    "> ❓ We have perform every single test with a hold-out strategy, however, as it was appointed in a previous session, the application of a cross-validation approach is prefered to cut the dependency on the selection of the samples. In this case you could think that there are two different approaches one is apply the cross-validation to each model, choose the better one and combine those in a single ensemble. The other way arround would be applying the cross-validation at ensemble level before training the models. Which one is correct and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fcbb5",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2b304",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "This last approach to combining the models can be considered as a variant of Soft Voting. As mentioned in that section, soft voting allows the weights of each of the models to be fixed and this can be adjusted with a decaying gradient technique. Stacking is usually identified as creating a classification technique superior to a linear regression (which is what Soft Voting does) such as an ANN to combine the models.\n",
    "\n",
    "Thus, as has been done previously, the outputs of the different techniques could be taken and used as inputs to another classification model, allowing for the adjustment of the weights and the non-linear combinations of the responses of each one.\n",
    "\n",
    "You can see an example or this in the following code, which uses the implementation on `MLJ` whcih uses an SVC as compbining model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ac43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NamedTuple of base models\n",
    "base_models_NamedTuple = (; (Symbol(name) => model for (name, model) in models)...)\n",
    "\n",
    "# Build the staking model\n",
    "# - resampling=CV(...) define how the out-of-boxfold predictions are generated\n",
    "# - measures=... only for internal reporting; does not affect final stack training\n",
    "models[\"Ensemble (Stacking)\"] = Stack(; \n",
    "    metalearner = SVC(),\n",
    "    resampling = CV(nfolds=5, shuffle=true, rng=123),\n",
    "    measures = log_loss,\n",
    "    base_models_NamedTuple...  # expands the named tuple of base models\n",
    ")\n",
    "\n",
    "# Train the stacking model on your train dataset\n",
    "machines[\"Ensemble (Stacking)\"] = machine(stack, train_input, train_output) |> fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed598b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9273829",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "\n",
    "One of the key elements that has not yet been addressed is the creation of the models that will compose the meta-classifier. So far, the approach that has been followed is not very adequate as the input dataset for all models is the same. This has the effect of an obvious lack of diversity in the models since whichever model we create, it will have the same information or \"point of view\" as the others. However, this is not the usual practice. Instead, the set of input patterns is usually divided into smaller sets with which to train one or more techniques in order to reduce the computational cost on the one hand, and to increase the diversity of the models on the other. It is necessary to remember at this point that \"weak\" models do not have to be perfect in all classes and do not even have to cover all possibilities, only models that are quick to train and offer a more or less consistent output.\n",
    "\n",
    "As for the way in which to partition the data for the creation of the models, most approaches usually consider two main approaches known as *Bagging* and *Boosting*. In the following, these two approaches will be briefly described.\n",
    "\n",
    "### Bagging or boostrap aggregation\n",
    "The technique known as _Bagging_ or selection with replacement was proposed by Breitman in 1996. It is based on the development of multiple models which can be trained in parallel. The key element of these models is that each model is trained on a subset of the training set. This subset of data is drawn randomly with replacement. This last point is particularly important because once an example has been selected from the possibilities, it is placed back among the possibilities so that it can be selected either in the subset being built, or in the subsets of the other models, i.e. non-disjoint sets of examples are created.\n",
    "\n",
    "![Bagging Example](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/440px-Ensemble_Bagging.svg.png)\n",
    "\n",
    "The result is that \"experts\" are created on specialised data and depending on the partition. While common, or more frequent, data is correctly covered by all models, it is also true that less frequent data tends not to be in all partitions and may not be covered in all cases. Thus, you would get models that would be more specialised in certain data or have a different point of view, that would be experts in a particular region of the search space.\n",
    "\n",
    "Although it will be discussed in more detail later, a well-known technique that uses this approach for the construction of its \"weak\" models is RandomForest. It builds the decision trees that make up the metaclassifier in this way. Any classifier can be used as the basis of a *Bagging* with the class [EnsembleModel](https://juliaai.github.io/MLJ.jl/stable/models/EnsembleModel_MLJEnsembles/#EnsembleModel_MLJEnsembles). \n",
    "\n",
    "For example, in the following code, 10 SVM for classication has been chosen as weak models. Each of those models habe been trained on only 50% of the training patterns, and therefore the variance among them should be increased.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7750d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Bagging model using SVC as base model\n",
    "using MLJEnsembles: EnsembleModel, CPUThreads\n",
    "\n",
    "models[\"Bagging (SVC)\"] = EnsembleModel(\n",
    "    model = SVC(),              # or ProbabilisticSVC()\n",
    "    n = 10,                     # number of base models\n",
    "    bagging_fraction = 0.50,    # fraction of examples per base model\n",
    "    rng = 123,                  \n",
    "    acceleration = CPUThreads() # uses Threads to speed up the training due to the independence of base models\n",
    ")\n",
    "\n",
    "machines[\"Bagging (SVC)\"] = machine(models[\"Bagging (SVC)\"], train_input, train_output) |> fit!\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5787a56",
   "metadata": {},
   "source": [
    "As an alternative to extracting complete examples, a vertical partition of the training _dataset_ could be performed, thus extracting features. To implement this alternative, in the `EnsembleModel` function, the parameter *bagging_fraction* must be defined. This approach is used when the number of features is particularly high in order to create simpler models that do not use all the information that is often redundant. It should be noted that this feature extraction procedure for models is done without replacement, i.e. features extracted for one classifier are not re-entered into the list of possibilities until the set for the next classifier is created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0484f377",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "The other major family of techniques for ensemble metamodelling is what is known as *Boosting*. In this case, the approach is slightly different, since the aim is to create a chain of classifiers. The key idea is that every successive classifier becomes **more specialized in the patterns that previous models misclassified**. Therefore, as in the previous case, a subset of patterns is selected from the original set. However, this process is done sequentially and without replacement. This causes the new learners to focus increasingly on the difficult cases, gradually producing a stronger and more accurate composite model. Thus, as in *Bagging*, the underlying idea of this approach is that not all models have to have all patterns as a basis, but unlike _Bagging_, this process is linear because of the dependency in the construction of the models. In the end, the outputs of the individual models are combined through a **weighted majority vote**, where each classifier’s weight reflects its performance during training.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Ensemble_Boosting.svg/1920px-Ensemble_Boosting.svg.png\" alt=\"Boosting examples\" width=\"600\"/>\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "The **AdaBoost** algorithm starts by assigning equal weights to all instances in the training set. A simple classifier (called a *stump*, which is a tree of only one level) is trained, and its performance is evaluated. Instances that are misclassified are given higher weights, so that the next classifier focuses more on those difficult cases.  This iterative process continues, each time updating the weights and creating a new weak learner that complements the previous ones. In AdaBoost, the weighting of both instances and classifiers is based on an **exponential loss function**, which penalizes misclassifications exponentially. The final ensemble prediction is obtained through a **weighted majority vote** among all weak classifiers. In MLJ, this behaviour is implemented by the `AdaBoostStumpClassifier`, provided by the `DecisionTree.jl` package.\n",
    "\n",
    "\n",
    "#### Gradient Boosting\n",
    "\n",
    "**Gradient Boosting** follows a different principle: instead of reweighting instances explicitly, it uses a **gradient descent approach** to minimize a loss function. Each new tree in the sequence is trained to predict the **residual errors** (or gradients) of the previous ensemble, gradually refining the model. In the case of classification, each decision tree models the **logistic likelihood** of the data, and its predictions are combined to estimate the class probabilities. The final decision is based on the sum of these probabilities across all trees. In MLJ, this can be implemented using the `EvoTreeClassifier` (from `EvoTrees.jl`), which is conceptually similar to scikit-learn’s `GradientBoostingClassifier`, but is written entirely in Julia and supports both CPU and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a90e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "\n",
    "# Load model (pure Julia implementations)\n",
    "AdaBoostStumpClassifier = @load AdaBoostStumpClassifier pkg=DecisionTree\n",
    "EvoTreeClassifier = @load EvoTreeClassifier pkg=EvoTrees\n",
    "\n",
    "# AdaBoost (similar to sklearn AdaBoostClassifier with stumps)\n",
    "models[\"AdaBoost\"] = AdaBoostStumpClassifier(n_iter = 30)\n",
    "machines[\"AdaBoost\"] = machine(models[\"AdaBoost\"], train_input, train_output) |> fit!(\n",
    "\n",
    "\n",
    "# Gradient Boosting (similar to sklearn GradientBoostingClassifier)\n",
    "models[\"EvoTrees\"] = EvoTreeClassifier(\n",
    "    nrounds=30,\n",
    "    eta=1.0,\n",
    "    max_depth=2,\n",
    "    loss=:logistic\n",
    ")\n",
    "\n",
    "machines[\"EvoTrees\"] = machine(models[\"EvoTrees\"], train_input, train_output) |> fit!()\n",
    "\n",
    "\n",
    "for (name, machine) in machines\n",
    "    acc = MLJ.accuracy(predict_mode(machine, test_input), test_output)\n",
    "    println(\"$name: $(acc*100) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43fc04b",
   "metadata": {},
   "source": [
    "### Question 7.3\n",
    "\n",
    "Develop a function to **train and evaluate a homogeneous ensemble** using **`EnsembleModel`** from **MLJ**, where all base estimators are of the **same type** (e.g., SVC, DecisionTree, etc.).  \n",
    "The function, named `trainClassEnsemble`, should follow a **stratified cross-validation** scheme and return at least the test metric(s) value.\n",
    "\n",
    "**Requirements and Steps (summary):**\n",
    "\n",
    "1. **Results vector:** Create a vector of length *k* to store, for each fold, the test metric(s) values.  \n",
    "2. **k-fold loop:** For each iteration, and using the provided stratified indices, build the four data partitions:  \n",
    "   - `X_train`, `y_train`, `X_test`, `y_test`.  \n",
    "3. **Base model generation:** Instantiate the **base model** (from Unit 6) with its **hyperparameters**.  \n",
    "4. **Build the homogeneous ensemble:** Wrap the base model inside an `EnsembleModel` (MLJ), configuring the number of models (`n`) and, if applicable, bagging parameters (`bagging_fraction`, `sampling_fraction`, `n_subfeatures`, etc.).  \n",
    "5. **Training:** Fit the ensemble on the training set (`machine`, `fit!`).  \n",
    "6. **(Optional) Internal validation:** If a validation set is required (e.g., for early stopping), apply a **hold-out** split on `X_train`, `y_train`.  \n",
    "7. **Evaluation:** Compute the chosen metric(s) on the test set for the current fold and store the results.  \n",
    "8. **Aggregation:** Finally, report the **average** and **standard deviation** of each metric across all folds.\n",
    "\n",
    "> **Important:** You must implement this as a **homogeneous ensemble** (all base learners of the same type) using **`EnsembleModel` from MLJ** (bagging approach). *Stacking* or *boosting* is not required here.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55290ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassEnsemble(estimator:Symbol, \n",
    "        modelsHyperParameters:: Dict,\n",
    "        ensembleHyperParameters:: Dict,     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "        kFoldIndices::     Array{Int64,1})\n",
    "    #TODO\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a265be0c",
   "metadata": {},
   "source": [
    "### Question 7.4\n",
    "> ❓ Repeated the previous function, but this time for a heterogeneous Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d7aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "function trainClassEnsemble(estimators:AbstractArray{Symbol, 1}, \n",
    "        modelsHyperParameters:: AbstractArray{Dict, 1},\n",
    "        ensembleHyperParameters:: Dict,     \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}},    \n",
    "        kFoldIndices::     Array{Int64,1})\n",
    "    #TODO\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154068c",
   "metadata": {},
   "source": [
    "## Techniques integrating the Ensemble approach\n",
    "\n",
    "Some of the best-known and currently used algorithms are based on this type of approach. Among these approaches, perhaps the most famous and widely used are those based on the generation of simple Decision Tress (DT). The reason for the use of the trees is their easy interpretation, as well as the speed of calculation and training. In the following we will see the two approaches known today in this sense, ***Random Forest*** and ***XGBoost***.\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "The **Random Forest** algorithm, proposed by Breiman and Cutler in 2006 (building upon an earlier idea by Ho in 1995, known as *Random Subspaces*), is one of the most representative examples of ensemble learning. It combines multiple simple classifiers — in this case, **Decision Trees (DTs)** — into a single, more robust model. Each tree in the forest is trained on a **bootstrap sample** (a random subset with replacement) of the original data, following a *bagging* approach. Because each tree is trained independently, the process can be fully **parallelized**. For classification problems, the final prediction is obtained by **majority vote** among all trees; for regression, by averaging their outputs.\n",
    "\n",
    "Random Forests are known for performing remarkably well with **minimal hyperparameter tuning**. Typically, the most important parameter is the number of trees (`n_trees` in MLJ or `n_estimators` in scikit-learn), which controls the size of the ensemble. A common heuristic suggests using:\n",
    "\n",
    "- *$\\sqrt{\\textrm{\\#feature}}$* for classification tasks  \n",
    "- *$\\frac{\\textrm{\\#feature}}{3}$* for regression tasks  \n",
    "\n",
    "Although increasing the number of trees usually improves performance, it tends to **saturate** beyond 500–1000 trees in most practical cases.\n",
    "\n",
    "In addition to the bootstrapping process, Random Forests introduce a **second level of randomness**: at each node split, only a random subset of features is considered as candidates for partitioning. This enhances the **diversity of the trees** and helps reduce the model’s variance while maintaining strong predictive power. An important byproduct of this mechanism is the ability to **quantify feature importance**. By analysing how much each variable contributes to the reduction of node impurity across all trees, Random Forests can estimate the relative importance of each feature.  \n",
    "This impurity-based importance is often used for **feature selection**. The most common impurity metric is the **Gini index**, defined as:\n",
    "\n",
    "$$G = \\sum_{i=1}^C p(i) * (1 - p(i))$$\n",
    "\n",
    "where $ C $ is the number of classes and $ p(i) $ is the probability of randomly selecting an instance of class $ i $.  Intuitively, it measures the probability of incorrectly classifying a randomly chosen instance if labels were assigned according to the class distribution. For an excellent visual explanation, see [this reference](https://victorzhou.com/blog/gini-impurity/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668be4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using MLJBase: accuracy\n",
    "using Plots\n",
    "\n",
    "# Load the native Julia Random Forest model\n",
    "RandomForestClassifier = @load RandomForestClassifier pkg=DecisionTree\n",
    "\n",
    "# Define the model\n",
    "models[\"RF\"] = RandomForestClassifier(\n",
    "    n_trees=8,              \n",
    "    max_depth=-1,           \n",
    "    min_samples_split=2,\n",
    "    n_subfeatures=-1,       \n",
    "    sampling_fraction=1.0   \n",
    ")\n",
    "\n",
    "# Train The modeel\n",
    "machines[\"RF\"] = machine(models[\"RF\"], train_input, train_output) |> fit!\n",
    "\n",
    "    \n",
    "# Evaluate accuracy\n",
    "for (name, mach) in machines\n",
    "    acc = accuracy(predict_mode(mach, test_input), test_output)\n",
    "    println(\"$name: $(round(acc*100, digits=2)) %\")\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dbe372",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "| **Parameter**        | **Description** |\n",
    "|-----------------------|-----------------|\n",
    "| `n_trees`             | Number of trees in the forest (equivalent to `n_estimators` in scikit-learn). |\n",
    "| `max_depth`           | Maximum depth of each tree. Use `-1` for no limit. |\n",
    "| `min_samples_split`   | Minimum number of samples required to split a node. |\n",
    "| `n_subfeatures`       | Number of random features considered at each split (`-1` uses √(#features)). |\n",
    "| `sampling_fraction`   | Fraction of training samples used to build each tree (bootstrapping). |\n",
    "| `rng`                 | Random number generator for reproducibility. |\n",
    "\n",
    "In this example, the number of trees (`n_trees`) is defined following the heuristic of $\\sqrt{\\textrm{\\#features}}$. Because the dataset used in this example is relatively small, results may vary slightly between runs depending on the random partitions used for training.\n",
    "\n",
    "#### Feature Importance\n",
    "Once the model has been trained, we can compute the feature importance based on the average Gini impurity reduction across all trees in the forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85f5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitted model parameters\n",
    "fitted_model = fitted_params(machines[\"RF\"])\n",
    "\n",
    "# Obtain feature importance values\n",
    "feature_importances = fitted_model.features_importance\n",
    "\n",
    "# Plot feature importance\n",
    "p = bar(\n",
    "    1:length(feature_importances),\n",
    "    feature_importances,\n",
    "    orientation = :horizontal,\n",
    "    legend = false\n",
    ")\n",
    "xlabel!(p, \"Gini Gain\")\n",
    "ylabel!(p, \"Feature\")\n",
    "title!(p, \"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feb1ec",
   "metadata": {},
   "source": [
    "As shown in the plot, most of the predictive information may be concentrated in a small number of features.\n",
    "Therefore, this metric can also be used for feature filtering or selection, which will be discussed in subsequent sections.\n",
    "\n",
    "Random Forests represent one of the most robust and widely used ensemble methods.\n",
    "They leverage bagging and feature randomness to build diverse trees, reducing variance and improving generalisation.\n",
    "Moreover, they naturally provide interpretable measures such as feature importance, making them not only powerful predictors but also useful tools for exploratory data analysis.\n",
    "\n",
    "### XGBoost (eXtreme Gradient Boosting)\n",
    "\n",
    "Finally, in this last section, Gradient Boosting should be mentioned again, specifically an implementation that in recent years has become very famous for its versatility and speed. This implementation is known as ***XGBoost (eXtreme Gradient Boosting)*** , which has stood out especially in competitions such as the Kaggle platform for its speed in obtaining results and robustness. \n",
    "\n",
    "The ***XGBoost*** will be a similar ensemble to Random Forest but uses a different base classifier known as CART (classification and regression trees) instead of *Decision Trees*. This change comes from the need for the algorithm to obtain the probability of the decisions, as was the case with *Gradient Tree Boosting*. The other fundamental change in this algotimo, since it is based on *Gradient Tree Boosting*, is the change from *bagging* to *boosting* strategy for the creation of the classifier training sets.\n",
    "\n",
    "Subsequently, this technique performs an additive training approach whose weights are adjusted based on a **Declining Gradient** on a *loss* function to be defined. By adding the *loss* function with the regularisation term, the second derivative of the functions can be calculated in order to update the classification weights of the different trees. The calculation of this gradient thus allows the adjustment of the values of the classifiers that are generated following a given one in order to allow the weights to focus attention on the patterns that are incorrectly classified. The mathematical details of the implementation can be found in this [link](https://xgboost.readthedocs.io/en/stable/tutorials/model.html).\n",
    "\n",
    "Unlike the other approaches we have seen, the `xgboost` is not currently implemented in `scikit learn` but it is integrated in MLJ. However, the reference version must be installed if it is not already present on the machine which is the one used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03972c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;\n",
    "Pkg.add(\"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e0c0f",
   "metadata": {},
   "source": [
    "After that installation, the library could be used as shown in the following example. Unlike other implementations, the Julia implementation supports Julia Array, SparseMatrixCSC, libSVM format text and XGBoost binary file as input.  Althouugh the vastly options given by Julia libraría in deep to change internaly to the format [LIBSVM](https://xgboost.readthedocs.io/en/stable/tutorials/input_format.html) as any other library. This library has not all the posibilities and, more especificl, the BitVector is not supported nowadays in their function `DMatrix`. So, an small change in the format is required in order to use the library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using XGBoost;\n",
    "\n",
    "train_input = input_data\n",
    "train_output = output_data\n",
    "\n",
    "test_input = input_data\n",
    "test_output = output_data\n",
    "\n",
    "train_output_asNumber= Vector{Number}(train_output);\n",
    "\n",
    "@assert train_output_asNumber isa Vector{Number}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad57fb4",
   "metadata": {},
   "source": [
    "Once this data adaptation is done, you can proceed with the training of a model from the `xgboost` library. To do so, it is only necessary to call the function train with the corresponding parameters. Among these parameters, the most important are:\n",
    "\n",
    "- **eta**, term that will determine the compression of the weights after each new stage of *boosting*. It takes values between 0 and 1.\n",
    "- **max_depth**, maximum depth of the trees has by default a value of 6, increasing it will allow more complex models.\n",
    "- **gamma**, parameter that controls the minimum loss reduction necessary to perform a new partition on a leaf node of the tree. The higher it is, the more conservative it will be\n",
    "- **alpha** and **lambda**, are the parameters that control the L1 and L2 regulation respectively.\n",
    "- **objective**, sets the loss function to be used which can be one of the predefined ones, which can be consulted in this [link](https://xgboost.readthedocs.io/en/stable/parameter.html#parameters-for-tree-booster)\n",
    "\n",
    "Further it is only necessary to set the maximum number of iterations of the boosting process as shown in the following example with 20 rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec905d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_data = DMatrix(train_input, label=train_output_asNumber)\n",
    "\n",
    "model = xgboost(svm_data, rounds=20, eta = 1, max_depth = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275ed94",
   "metadata": {},
   "source": [
    "In the following piece of code, several parameters as passed as a dictionary and two different metrics are calculated. First, error refers to the incorrectly classified ones over the total amount, and the second one is the Area Under the Curve ROC (AUC).\n",
    "\n",
    "### Question 7.5\n",
    "> ❓ Which is the canonical name of the first measure that is been monitored?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f5411",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ab769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [\"max_depth\" => 2,\n",
    "         \"eta\" => 1,\n",
    "         \"objective\" => \"binary:logistic\"]\n",
    "metrics = metrics = [\"error\", \"auc\"]\n",
    "model = xgboost(DMatrix(train_input, label=train_output_asNumber), rounds=20, param=param, metrics=metrics)\n",
    "\n",
    "pred = predict(model, train_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120deba",
   "metadata": {},
   "source": [
    "***Important***.\n",
    "\n",
    "In case a validation set is used, this must be passed in the *evals* parameter of the training function. In addition, and only when the mentioned *evals* parameter is defined, you can set the rounds for the pre-stop with the *early_stopping_rounds* parameter of the training function. The code would be similar to:\n",
    "``` julia\n",
    "    evals = DMatrix(val_input, label=val_output)\n",
    "    xgb_model = xgb.train(param, train_input, num_round,label = train_output_asNumber, evals=evals,\n",
    "                    early_stopping_rounds=10)\n",
    "```\n",
    "\n",
    "The value provided in the output corresponds to the sum of the outputs of the trees, being between 0 and 1 for membership of a given class. Since this is a binary class, simply set a limit of 0.5 to the output to determine what the answer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a468354c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using XGBoost: predict as predict_xgb\n",
    "\n",
    "pred = predict_xgb(model, test_input)\n",
    "print(\"Error of XGboost= \", sum((pred .> 0.5) .!= test_output) / float(size(pred)[1]), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569d26f2",
   "metadata": {},
   "source": [
    "Finally, as in the case of the Random Forest it is possible to identify the importance and paint it for each of the variables in the ranking. With the following code it is possible to see such a marker ordered in a ascendent way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e86f1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_gain =  [(first(x),last(x)) for x in importance(model)]\n",
    "feature, gain = first.(feature_gain), last.(feature_gain)\n",
    "\n",
    "using Plots;\n",
    "\n",
    "p = bar(feature, y=gain, orientation=\"h\", legend=false)\n",
    "xlabel!(p,\"Gain\")\n",
    "ylabel!(p,\"Feature\")\n",
    "title!(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533db36",
   "metadata": {},
   "source": [
    "As you can see, not all features has the same importance. It should be notice that the Feature axis identifies the position in the Vector  feature which is ordered by the gain value by default.\n",
    "\n",
    "### Integration with MLJ\n",
    "\n",
    "As mentioned previously, **XGBoost** can be seamlessly integrated into the **MLJ framework**, allowing it to be used consistently with other models, pipelines, and tuning workflows.  \n",
    "This integration is provided through the [`MLJXGBoostInterface.jl`](https://github.com/JuliaAI/MLJXGBoostInterface.jl) package, which acts as a bridge between MLJ and the native XGBoost implementation.\n",
    "\n",
    "To load the model, simply use the `@load` macro from MLJ:\n",
    "\n",
    "```julia\n",
    "# Load data and ensure correct scientific types\n",
    "using DataFrames, MLJ\n",
    "\n",
    "# Load XGBoost classifier\n",
    "XGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n",
    "```\n",
    "Once loaded, the classifier can be configured, trained, and evaluated like any other MLJ model.\n",
    "\n",
    "```julia\n",
    "# Define model and parameters\n",
    "xgb_model = XGBoostClassifier(\n",
    "    num_round = 100,\n",
    "    eta = 0.1, \n",
    "    max_depth = 6,\n",
    "    objective = \"multi:softprob\"       # suitable for multi-class problems\n",
    ")\n",
    "\n",
    "# Bind model and data\n",
    "mach = machine(xgb_model, X, y)\n",
    "\n",
    "# Train the model\n",
    "fit!(mach, verbosity = 1)\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "cv_result = evaluate!(\n",
    "    mach,\n",
    "    resampling = CV(nfolds = 5, shuffle = true),\n",
    "    measure = [accuracy, cross_entropy],\n",
    "    verbosity = 0\n",
    ")\n",
    "\n",
    "cv_result.measurement\n",
    "\n",
    "# ==============================\n",
    "# Example of Tuning of the model\n",
    "# ==============================\n",
    "\n",
    "# Define a parameter range to explore\n",
    "r_eta = range(xgb_model, :eta, lower=0.01, upper=0.3)\n",
    "r_depth = range(xgb_model, :max_depth, lower=3, upper=10)\n",
    "\n",
    "# Define tuning strategy\n",
    "tuned_xgb = TunedModel(\n",
    "    model = xgb_model,\n",
    "    resampling = CV(nfolds=5, shuffle=true),\n",
    "    range = [r_eta, r_depth],\n",
    "    measure = accuracy,\n",
    "    tuning = Grid(resolution=5),\n",
    "    operation = predict_mode\n",
    ")\n",
    "\n",
    "# Train tuned model\n",
    "mach_tuned = machine(tuned_xgb, X, y)\n",
    "fit!(mach_tuned)\n",
    "\n",
    "```\n",
    "\n",
    "## Julia Notes\n",
    "\n",
    "### Understanding `coerce` in MLJ\n",
    "\n",
    "When working with datasets in MLJ, it’s important to understand that MLJ distinguishes between **machine types** (e.g., `Int64`, `Float64`, `String`) and **scientific types** (or `scitypes`), which describe how data should be *interpreted* for modeling.\n",
    "\n",
    "### Why `coerce` is Needed\n",
    "\n",
    "MLJ models don’t rely on the raw Julia types — instead, they expect variables to have *scientific meanings*:\n",
    "- A numeric column can represent a **continuous** feature.\n",
    "- A string column can represent a **categorical** feature.\n",
    "- A boolean or integer can be **ordered** or **unordered**, depending on context.\n",
    "\n",
    "Since Julia doesn’t know this automatically, we use the `coerce()` function to explicitly tell MLJ how to interpret each column.  \n",
    "This ensures compatibility between your data and the MLJ model you plan to use.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ, DataFrames, CSV\n",
    "\n",
    "# Load the dataset\n",
    "data = CSV.read(\"sonar.csv\", DataFrame)\n",
    "\n",
    "# Inspect current column types\n",
    "schema(data)\n",
    "\n",
    "# Suppose the last column is the target ('Rock' or 'Mine')\n",
    "y, X = unpack(data, ==(:Target), rng=123)\n",
    "\n",
    "# Convert the target variable to categorical\n",
    "y = coerce(y, Multiclass)\n",
    "\n",
    "# Convert all feature columns to continuous\n",
    "X = coerce(X, autotype(X, rules = (:discrete_to_continuous,)))\n",
    "\n",
    "# Verify the new scientific types\n",
    "schema(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39a8f6",
   "metadata": {},
   "source": [
    "\n",
    "### Common examples\n",
    "\n",
    "| Situation                            | What to Do              | Example                              |\n",
    "| ------------------------------------ | ----------------------- | ------------------------------------ |\n",
    "| Categorical labels stored as strings | Convert to `Multiclass` | `y = coerce(y, Multiclass)`          |\n",
    "| Numerical features                   | Convert to `Continuous` | `X = coerce(X, :var1 => Continuous)` |\n",
    "| Automatic inference                  | Use `autotype()`        | `autotype(X)`                        |\n",
    "| Check current scientific types       | Use `schema()`          | `schema(X)`                          |\n",
    "\n",
    "### What Happens If You Skip coerce\n",
    "\n",
    "If you skip the coercion step:\n",
    "\n",
    "* MLJ might misinterpret your target as continuous, blocking classification models.\n",
    "* Some algorithms (e.g., DecisionTree, NaiveBayes) may fail or produce wrong predictions.\n",
    "* The machine() function might refuse to bind the model and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b376d4",
   "metadata": {},
   "source": [
    "## Voting Classifier\n",
    "\n",
    "Following this lines there are several examples of how to use the new Voting Classifier that we have implemented.\n",
    "\n",
    "In previous examples, we created the ensemble as follows:\n",
    "\n",
    "```julia\n",
    "voting_hard = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "voting_soft = VotingClassifier(models=base_models_list, voting=:soft)\n",
    "\n",
    "mach_hard = machine(voting_hard, train_input, train_output) |> fit!\n",
    "mach_soft = machine(voting_soft, train_input, train_output) |> fit!\n",
    "```\n",
    "\n",
    "* `voting=:hard` → uses majority voting (based on class labels).\n",
    "\n",
    "* `voting=:soft` → uses average probability voting (requires models that output probabilities).\n",
    "\n",
    "The following examples show how to modify and integrate the VotingClassifier dynamically within your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45bc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Changing the Voting Type Dynamically\n",
    "ensemble = VotingClassifier(models=base_models_list, voting=:hard)\n",
    "println(\"\\nCurrent voting type: $(ensemble.voting)\")\n",
    "\n",
    "ensemble.voting = :soft\n",
    "println(\"Voting type changed to: $(ensemble.voting)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a28c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Using the VotingClassifier in a Pipeline\n",
    "pipe_hard = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:hard)\n",
    ")\n",
    "\n",
    "pipe_soft = @pipeline(\n",
    "    Standardizer(),\n",
    "    VotingClassifier(models=base_models_list, voting=:soft)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade52dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Cross-Validation Comparing Both Voting Strategies\n",
    "cv_hard = evaluate!(\n",
    "    machine(voting_hard, train_input, train_output),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=accuracy\n",
    ")\n",
    "println(\"Hard Voting CV accuracy: $(round(mean(cv_hard.measurement)*100, digits=2)) %\")\n",
    "\n",
    "cv_soft = evaluate!(\n",
    "    machine(voting_soft, train_input, train_output),\n",
    "    resampling=CV(nfolds=5),\n",
    "    measure=accuracy\n",
    ")\n",
    "println(\"Soft Voting CV accuracy: $(round(mean(cv_soft.measurement)*100, digits=2)) %\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
